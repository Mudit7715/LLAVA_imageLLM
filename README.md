# LLAVA_imageLLM
Using a local MultiModal LLM Llava for image-to-voice conversions

## Multimodal LLM Application: Image-to-Voice Conversion

This document outlines the components, considerations, and setup for creating an application that converts images to voice using local Large Language Models (LLMs).

### Overview

The application integrates three primary functions:

1.  **Speech-to-Text (STT):**  Transcribes user's voice input.
2.  **Text Generation:**  Analyzes image (optionally with STT transcript) and generates a text description.
3.  **Text-to-Speech (TTS):**  Converts generated text to audio output.

### Core Components

#### 1. Model Quantization (Bitsandbytes)

*   **Overview:** Reduces the memory footprint of LLMs by using lower-precision number formats (e.g., 4-bit instead of 32-bit).  This allows larger models to run on less powerful hardware like Google Colab.
*   **Application:** Loading the LLaVA model with 4-bit quantization.
*   **Configuration:**
    *   `load_in_4bit=True`  (enables 4-bit quantization)
    *   `bnb_4bit_compute_dtype=torch.float16` (reduces computational overhead during quantization)
*   **Pros:**
    *   Reduces memory usage.
    *   Enables running large models on resource-constrained environments.
*   **Cons:**
    *   Potential reduction in model accuracy compared to full precision.

#### 2. Speech-to-Text (Whisper)

*   **Overview:** An ASR model by OpenAI designed to transcribe multilingual speech with high accuracy, even in noisy environments and with diverse accents.
*   **Application:** Converts raw audio input from the user into text for processing by LLaVA.
*   **Key Features:**
    *   High accuracy across multiple languages.
    *   Robustness to noise and varying audio quality.
    *   Handles different accents and speech patterns.
*   **Note:** This process often works with audio *mel spectrograms* rather than raw audio data for better human speech processing

#### 3. Text-to-Speech (gTTS)

*   **Overview:** A lightweight Google Text-to-Speech tool used to convert generated text responses into audio.
*   **Application:** Converts the text description generated by LLaVA into speech for audio-based output.
*   **Pros:**
    *   Low latency.
    *   Minimal resource requirements.
*   **Cons:**
    *   Limited customization options for voice and intonation.

### Final Model Selection

*   **Speech-to-Text:** Whisper
*   **Text Generation:** LLaVA
*   **Text-to-Speech:** gTTS

### Metrics and Hyperparameter Tuning

*   **Model Quantization:** Using Bitsandbytes to quantize from 16-bit to 4-bit.
*   **Compute Datatype:** Setting `bnb_4bit_compute_dtype=torch.float16` to reduce overhead.
*   **Device Setting:** Using CUDA GPU for faster computation (`DEVICE = "cuda" if torch.cuda.is_available() else "cpu"`).
*   **Audio Representation:**  Using Mel spectrograms for improved human voice perception.

### References

*   **Gradio:** [https://www.gradio.app/docs/gradio/interface](https://www.gradio.app/docs/gradio/interface)
*   **LLaVA:** [https://llava-vl.github.io/](https://llava-vl.github.io/)
*   **gTTS:** [https://pypi.org/project/gTTS/](https://pypi.org/project/gTTS/)
*   **Whisper:** [https://github.com/openai/whisper](https://github.com/openai/whisper)
*   **Bitsandbytes:** [https://pypi.org/project/bitsandbytes/](https://pypi.org/project/bitsandbytes/)
*   **Accelerate:** [https://huggingface.co/docs/accelerate/en/index](https://huggingface.co/docs/accelerate/en/index)

